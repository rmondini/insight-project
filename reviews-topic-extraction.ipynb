{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from gensim import models, corpora\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.matutils import cossim\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to clean text (removing stopwords, lemmatization, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "# remove additional generic words\n",
    "STOPWORDS = STOPWORDS + ['also','anything','area','around','ask','back','bad','basic','better','bit','cant',\n",
    "                         'complaint','cost','could','day','didnt','disgusting','dont','even','ever','everything',\n",
    "                         'extra','feel','get','good','great','horrible','hotel','hour','leave','like','line',\n",
    "                         'little','look','money','much','need','never','new','night','nothing','ok','old','one',\n",
    "                         'option','overall','paid','pay','per','picture','place','price','quite','rate','really',\n",
    "                         'recommend','said','say','see','service','stay','stayed','sure','terrible','thing',\n",
    "                         'think','time','time','told','took','turn','u','use','want','wasnt','way','well','went',\n",
    "                         'work','worst','worth','would']\n",
    "\n",
    "# initialize lemmatizer and stemmer\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [wn_lemmatizer.lemmatize(p_stemmer.stem(token)) for token in tokenized_text if token not in STOPWORDS]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import negative sentences dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences = pd.read_csv('./datasets/df_negative_sentences.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences = df_negative_sentences[~pd.isnull(df_negative_sentences['review_sentence'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify set of topics and topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_comfort_topic = 'Noise noisy loud slam voice music thin wall hear' + ' Smell smelly odor stink' + ' Ac heat hot cold thermostat air vent ventilation fan adjust heater temperature'\n",
    "staff_topic = 'Staff rude unfriendly friendly polite front desk manager maid reception valet clerk housekeep waiter waitress' + ' Check checkin checkout communication experience bag early late reservation booking'\n",
    "breakfast_topic = 'Breakfast food egg bacon sausage toast potato waffle fruit omelette omelet cheese milk pastry coffee tea juice silverware plasticware cup plastic included selection taste fresh'\n",
    "facilities_topic = 'Facility elevator lift stair wheelchair pool jacuzzi gym vending spa sauna renovation bar restaurant lounge pet property' + ' WiFi internet connection' + ' Park lot car valet street'\n",
    "location_topic = 'Location surrounding far traffic highway walk street road neighborhood sketchy attraction center city town downtown nearby near walk transport subway park view safe dangerous drive'\n",
    "bathroom_topic = 'Bathroom shower tub bathtub drain curtain pressure sink water toiletry toilet mirror shampoo conditioner towel soap ply paper hair face wash vent ventilation fan window'\n",
    "room_amenities_topic = 'Room carpet curtain shade drape wardrobe outlet plug window tv balcony couch remote wall fridge refrigerator safe machine coffee tea kettle amenity microwave card door'\n",
    "bed_quality_topic = 'Bed thin sheet linen blanket cover comforter pillow hard soft firm mattress bug bedbug king double queen twin frame sleep pullout couch bunk comfortable flat'\n",
    "\n",
    "topics= [room_comfort_topic,staff_topic,breakfast_topic,facilities_topic,location_topic,bathroom_topic,room_amenities_topic,bed_quality_topic]\n",
    "n_topics = len(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table of topic and topic keywords for demo slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords_dict = {'topic': ['bathroom','bed quality','breakfast','facilities',\n",
    "                                 'location','room amenities','room comfort','staff'],\n",
    "                       'keywords': ['bathroom, drain, shower, tub, toilet, ...',\n",
    "                                    'bed, bug, mattress, pillow, sheet, ....',\n",
    "                                    'breakfast, coffee, egg, selection, tea, ...',\n",
    "                                    'bar, facility, gym, parking, pool, ...',\n",
    "                                    'far, location, safe, subway, walk, ...',\n",
    "                                    'balcony, couch, room, tv, wardrobe, ...',\n",
    "                                    'ac, adjust, heat, noise, smell, ...',\n",
    "                                    'manager, reception, rude, staff, valet, ...']}\n",
    "topic_keywords_df = pd.DataFrame.from_dict(topic_keywords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences into words and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_neg_data = []\n",
    "for text in df_negative_sentences['review_sentence']:\n",
    "    tokenized_neg_data.append(clean_text(text))\n",
    "    \n",
    "tokenized_topics = []    \n",
    "for text in topics:\n",
    "    tokenized_topics.append(clean_text(text))\n",
    "    \n",
    "tokenized_neg_data_and_topics = tokenized_neg_data + tokenized_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dictionary = corpora.Dictionary(tokenized_neg_data_and_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag-of-words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_neg_data = [neg_dictionary.doc2bow(text) for text in tokenized_neg_data]\n",
    "corpus_neg_topics = [neg_dictionary.doc2bow(text) for text in tokenized_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build similarity matrix of word embeddings from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "termsim_index = WordEmbeddingSimilarityIndex(fasttext_model300)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index,neg_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute (soft) cosine similarity between sentences and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data_topics = []\n",
    "for review_item in corpus_neg_data:\n",
    "    review_item_topics = []\n",
    "    for topic in corpus_neg_topics:\n",
    "        # using BoW vectors (cosine similarity)\n",
    "        #review_item_topics.append(cossim(review_item,topic))\n",
    "        # using BoW + word embeddings (soft cosine similarity)\n",
    "        review_item_topics.append(similarity_matrix.inner_product(review_item,topic,normalized=True))\n",
    "    neg_data_topics.append(review_item_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract topic with highest (soft) cosine similarity\n",
    "- We set a minimum threshold (0.05) that needs to be reached in order to assign a topic\n",
    "- If above-threshold topics are within 0.002, we assign -1 (i.e. no main topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data_closest_topic = []\n",
    "cossim_threshold = 0.05\n",
    "\n",
    "for review_item_topic_list in neg_data_topics:\n",
    "    if max(review_item_topic_list)>cossim_threshold:\n",
    "        review_item_array = np.array(review_item_topic_list)\n",
    "        sorted_review_item_array = sorted(review_item_array,reverse=True)\n",
    "        num_topics=1\n",
    "        for item in sorted_review_item_array[1:]:\n",
    "            if abs(item-sorted_review_item_array[0])<0.002:\n",
    "                num_topics+=1\n",
    "        if num_topics==1:\n",
    "            neg_data_closest_topic.append(np.argmax(review_item_topic_list))\n",
    "        else:\n",
    "            neg_data_closest_topic.append(-1)\n",
    "    else:\n",
    "        neg_data_closest_topic.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign extracted topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences['review_topic'] = neg_data_closest_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    54422\n",
       " 6    52701\n",
       " 5    25216\n",
       " 1    24565\n",
       " 7    18235\n",
       " 2    18063\n",
       " 3    17588\n",
       " 0    15167\n",
       " 4    12231\n",
       "Name: review_topic, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize distribution of topics across corpus\n",
    "df_negative_sentences['review_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct pivot table of hotels (index), topics (columns), and topic counts (values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences_by_topic = df_negative_sentences.groupby(['hotel_url','review_topic']).size().reset_index()\n",
    "df_negative_sentences_by_topic.rename({0:'review_topic_count'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences_by_topic_pt = df_negative_sentences_by_topic.pivot_table(values='review_topic_count',index='hotel_url',columns='review_topic').reset_index()\n",
    "df_negative_sentences_by_topic_pt.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize each count by total number of negative sentences per hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences_count_by_hotel = df_negative_sentences.groupby('hotel_url').count().reset_index()[['hotel_url','review_topic']]\n",
    "df_negative_sentences_count_by_hotel.rename({'review_topic':'sentences_count'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences_by_topic_pt = df_negative_sentences_by_topic_pt.merge(df_negative_sentences_count_by_hotel,on='hotel_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns with normalized topic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences_by_topic_pt[[str(n)+'_pc' for n in range(-1,n_topics)]] = df_negative_sentences_by_topic_pt[[n for n in range(-1,n_topics)]].div(df_negative_sentences_by_topic_pt.sentences_count, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write pivot table out to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_sentences_by_topic_pt.to_csv('./datasets/df_negative_sentences_by_topic_pt_we.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate topic extraction against manually-annotated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading manually-annotated dataset\n",
    "df_negative_sentences_annotated = pd.read_csv('./datasets/df_negative_sentences_annotated_only.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only relevant columns\n",
    "df_negative_sentences_annotated = df_negative_sentences_annotated[['review_date','review_sentence','review_topic_annotated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes and keeping only entries with manually-annotated topic\n",
    "df_negative_sentences_topic_validation = df_negative_sentences.merge(df_negative_sentences_annotated,on=['review_date','review_sentence'])\n",
    "df_negative_sentences_topic_validation = df_negative_sentences_topic_validation[~pd.isnull(df_negative_sentences_topic_validation['review_topic_annotated'])]\n",
    "df_negative_sentences_topic_validation['review_topic_annotated'] = df_negative_sentences_topic_validation['review_topic_annotated'].apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.81      0.88       152\n",
      "           0       0.72      0.49      0.58        37\n",
      "           1       0.64      0.70      0.67        23\n",
      "           2       0.83      0.96      0.89        26\n",
      "           3       0.86      0.65      0.74        48\n",
      "           4       0.54      0.67      0.60        21\n",
      "           5       0.79      0.97      0.87        35\n",
      "           6       0.52      0.74      0.61        46\n",
      "           7       0.72      1.00      0.84        23\n",
      "\n",
      "    accuracy                           0.77       411\n",
      "   macro avg       0.73      0.78      0.74       411\n",
      "weighted avg       0.80      0.77      0.78       411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report on manually-annotated dataset\n",
    "print(classification_report(df_negative_sentences_topic_validation['review_topic_annotated'],df_negative_sentences_topic_validation['review_topic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create f1-score dataframe for demo slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_dict = {'topic': ['bathroom','bed quality','breakfast','facilities',\n",
    "                           'location','room amenities','room comfort','staff','no main topic'],\n",
    "                 'f1-score': [0.87,0.84,0.89,0.74,0.60,0.61,0.58,0.67,0.88]}\n",
    "f1_score_df = pd.DataFrame.from_dict(f1_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create topic pie chart for demo slides  \n",
    "(pie chart commented out for readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_counts = df_negative_sentences['review_topic'].value_counts().rename_axis('Topic').reset_index(name='Count')\n",
    "df_topic_counts.replace(-1,'no main topic',inplace=True)\n",
    "df_topic_counts.replace(0,'room comfort',inplace=True)\n",
    "df_topic_counts.replace(1,'staff',inplace=True)\n",
    "df_topic_counts.replace(2,'breakfast',inplace=True)\n",
    "df_topic_counts.replace(3,'facilities',inplace=True)\n",
    "df_topic_counts.replace(4,'location',inplace=True)\n",
    "df_topic_counts.replace(5,'bathroom',inplace=True)\n",
    "df_topic_counts.replace(6,'room amenities',inplace=True)\n",
    "df_topic_counts.replace(7,'bed quality',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# df_topic_counts['Count'].plot(kind='pie', labels=df_topic_counts['Topic'], autopct='%1.1f%%', pctdistance=0.81, labeldistance=1.10, startangle=90, shadow=False, legend = False, fontsize=20)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove additional generic words\n",
    "STOPWORDS_LDA = STOPWORDS + ['call','charge','charge','clean','cleaned','cold','dirty','filthy','hot','lot',\n",
    "                             'need','people','poor','room','rooms','small','u','us','1']\n",
    "\n",
    "def clean_text_lda(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [wn_lemmatizer.lemmatize(token) for token in tokenized_text if token not in STOPWORDS_LDA]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and tokenize\n",
    "tokenized_neg_data_lda = []\n",
    "for text in df_negative_sentences['review_sentence']:\n",
    "    tokenized_neg_data_lda.append(clean_text_lda(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and bag-of-words vectors\n",
    "neg_dictionary_lda = corpora.Dictionary(tokenized_neg_data_lda)\n",
    "corpus_neg_data_lda = [neg_dictionary_lda.doc2bow(text) for text in tokenized_neg_data_lda] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of topics\n",
    "NUM_TOPICS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the LDA model (multicore)\n",
    "lda_model = models.LdaMulticore(corpus=corpus_neg_data_lda, num_topics=NUM_TOPICS, id2word=neg_dictionary_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.094*\"breakfast\" + 0.025*\"air\" + 0.017*\"wifi\" + 0.013*\"bar\" + 0.012*\"food\"\n",
      "Topic #1: 0.024*\"door\" + 0.022*\"noise\" + 0.021*\"breakfast\" + 0.017*\"location\" + 0.013*\"outside\"\n",
      "Topic #2: 0.018*\"check\" + 0.015*\"first\" + 0.015*\"parking\" + 0.013*\"light\" + 0.012*\"card\"\n",
      "Topic #3: 0.084*\"bathroom\" + 0.029*\"coffee\" + 0.021*\"tv\" + 0.015*\"facility\" + 0.014*\"carpet\"\n",
      "Topic #4: 0.049*\"shower\" + 0.039*\"water\" + 0.029*\"bed\" + 0.028*\"towel\" + 0.025*\"floor\"\n",
      "Topic #5: 0.053*\"bed\" + 0.021*\"broken\" + 0.020*\"smelled\" + 0.017*\"got\" + 0.014*\"uncomfortable\"\n",
      "Topic #6: 0.067*\"staff\" + 0.032*\"desk\" + 0.028*\"front\" + 0.018*\"rude\" + 0.014*\"check\"\n",
      "Topic #7: 0.030*\"wall\" + 0.024*\"elevator\" + 0.020*\"window\" + 0.017*\"smell\" + 0.016*\"hallway\"\n"
     ]
    }
   ],
   "source": [
    "# print most representative words from extracted topics\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show example of extracted topics for demo slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics_dict = {'topic': ['topic #0','topic #1','topic #2','topic #3','topic #4'],\n",
    "                 'most representative words': ['0.094*breakfast + 0.025*air + 0.017*wifi + 0.013*bar + 0.012*food',\n",
    "                                               '0.024*door + 0.022*noise + 0.021*breakfast + 0.017*location + 0.013*outside',\n",
    "                                               '0.030*wall + 0.024*elevator + 0.020*window + 0.017*smell + 0.016*hallway',\n",
    "                                               '0.084*bathroom + 0.029*coffee + 0.021*tv + 0.015*facility + 0.014*carpet',\n",
    "                                               '0.049*shower + 0.039*water + 0.029*bed + 0.028*towel + 0.025*floor']}\n",
    "lda_topics_df = pd.DataFrame.from_dict(lda_topics_dict)\n",
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
